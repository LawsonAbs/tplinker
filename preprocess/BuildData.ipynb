{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "from common.utils import Preprocessor\n",
    "import yaml\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"build_data_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = config[\"exp_name\"]\n",
    "data_in_dir = os.path.join(config[\"data_in_dir\"], exp_name)\n",
    "data_out_dir = os.path.join(config[\"data_out_dir\"], exp_name)\n",
    "if not os.path.exists(data_out_dir):\n",
    "    os.makedirs(data_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../ori_data/webnlg\n"
     ]
    }
   ],
   "source": [
    "file_name2data = {}\n",
    "print(data_in_dir)\n",
    "for path, folds, files in os.walk(data_in_dir):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        file_name = re.match(\"(.*?)\\.json\", file_name).group(1)\n",
    "        file_name2data[file_name] = json.load(open(file_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @specific\n",
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] == \"BiLSTM\":\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = tokenize(text)\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-label', 'location', 17, 20, 'None-label'], [7, 10, 'None-label', 'cityServed', 3, 4, 'None-label'], [7, 10, 'None-label', 'elevationAboveTheSeaLevel_(in_metres)', 11, 12, 'None-label']], 'pos_tags': ['DT', 'NN', 'IN', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', 'JJ', 'CD', 'NNS', 'IN', 'NN', 'NN', 'IN', 'NNP', ',', 'NNP', '.', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'CD', 'CC', 'VBZ', 'CD', 'IN', 'NN', '.']}, {'tokens': ['The', 'city', 'of', 'Appleton', 'is', 'served', 'by', 'Appleton', 'International', 'Airport', 'in', 'Greenville', '(', 'part', 'of', 'Menasha', ')', ',', 'Clayton', 'Winnebago', 'County', ',', 'Wisconsin', ',', 'United', 'States', '.'], 'spo_list': [['Appleton International Airport', 'location', 'Greenville'], ['Greenville', 'isPartOf', 'Menasha'], ['Greenville', 'country', 'United States'], ['Appleton International Airport', 'cityServed', 'Appleton'], ['Greenville', 'isPartOf', 'Clayton']], 'spo_details': [[7, 10, 'None-label', 'location', 11, 12, 'None-label'], [11, 12, 'None-label', 'isPartOf', 15, 16, 'None-label'], [11, 12, 'None-label', 'country', 24, 26, 'None-label'], [7, 10, 'None-label', 'cityServed', 3, 4, 'None-label'], [11, 12, 'None-label', 'isPartOf', 18, 19, 'None-label']], 'pos_tags': ['DT', 'NN', 'IN', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', '-LRB-', 'NN', 'IN', 'NN', '-RRB-', ',', 'NNP', 'NNP', 'NNP', ',', 'NNP', ',', 'NNP', 'NNPS', '.']}, {'tokens': ['Ardmore', 'Airport', 'in', 'New', 'Zealand', 'is', 'operated', 'by', 'the', 'Civil', 'Aviation', 'Authority', 'and', 'is', '34', 'metres', 'above', 'sea', 'level', '.', 'It', 'has', 'a', 'runway', 'named', '03R/21L', 'which', 'is', '1,411', 'long', 'and', 'its', 'third', 'runway', 'has', 'a', 'poaceae', 'surface', '.'], 'spo_list': [['Ardmore Airport', 'operatingOrganisation', 'Civil Aviation Authority']], 'spo_details': [[0, 2, 'None-label', 'operatingOrganisation', 9, 12, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'CC', 'VBZ', 'CD', 'NNS', 'IN', 'NN', 'NN', '.', 'PRP', 'VBZ', 'DT', 'NN', 'VBN', 'NN', 'WDT', 'VBZ', 'CD', 'JJ', 'CC', 'PRP$', 'JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.']}, {'tokens': ['Turkmenistan', 'Airlines', 'operates', 'Ashgabat', 'International', 'Airport', 'in', 'Ashgabat', '.', 'The', 'airport', 'is', '211', 'metres', 'above', 'sea', 'level', ',', 'has', 'the', 'runway', 'length', 'of', '2989.0', 'and', '12R/30L', 'is', 'the', 'runway', 'name', '.'], 'spo_list': [['Ashgabat International Airport', 'operatingOrganisation', 'Turkmenistan Airlines'], ['Ashgabat International Airport', 'location', 'Ashgabat'], ['Ashgabat International Airport', 'runwayLength', '2989.0'], ['Ashgabat International Airport', 'elevationAboveTheSeaLevel_(in_metres)', '211']], 'spo_details': [[3, 6, 'None-label', 'operatingOrganisation', 0, 2, 'None-label'], [3, 6, 'None-label', 'location', 3, 4, 'None-label'], [3, 6, 'None-label', 'runwayLength', 23, 24, 'None-label'], [3, 6, 'None-label', 'elevationAboveTheSeaLevel_(in_metres)', 12, 13, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'VBZ', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', '.', 'DT', 'NN', 'VBZ', 'CD', 'NNS', 'IN', 'NN', 'NN', ',', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'NN', 'VBZ', 'DT', 'NN', 'NN', '.']}, {'tokens': ['Atlantic', 'City', 'International', 'Airport', 'has', 'a', 'runway', 'length', 'of', '1,873', 'and', 'is', 'operated', 'by', 'the', 'Port', 'Authority', 'of', 'New', 'York', 'and', 'New', 'Jersey', '.', 'It', 'is', 'situated', '23', 'metres', 'above', 'sea', 'level', ',', 'has', 'the', 'runway', 'name', 'of', '4/22', 'and', 'is', 'known', 'by', 'the', 'ICAO', 'location', 'identifier', 'KACY', '.'], 'spo_list': [['Atlantic City International Airport', 'operatingOrganisation', 'Port Authority of New York and New Jersey']], 'spo_details': [[0, 4, 'None-label', 'operatingOrganisation', 15, 23, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'VBZ', 'VBN', 'IN', 'DT', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', '.', 'PRP', 'VBZ', 'VBN', 'CD', 'NNS', 'IN', 'NN', 'NN', ',', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'CD', 'CC', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'NN', 'NN', 'NNP', '.']}, {'tokens': ['The', 'Flemish', 'Government', 'have', 'jurisdiction', 'over', 'the', 'Flemish', 'Region', 'of', 'Belgium', '.', 'The', 'leader', 'of', 'the', 'country', 'is', 'Charles', 'Michel', 'and', 'the', 'city', 'of', 'Antwerp', 'is', 'served', 'by', 'Antwerp', 'International', 'Airport', '.'], 'spo_list': [['Belgium', 'leaderName', 'Charles Michel'], ['Antwerp International Airport', 'cityServed', 'Antwerp'], ['Flemish Region', 'leaderName', 'Flemish Government'], ['Flemish Region', 'country', 'Belgium'], ['Flemish Government', 'jurisdiction', 'Flemish Region']], 'spo_details': [[10, 11, 'None-label', 'leaderName', 18, 20, 'None-label'], [28, 31, 'None-label', 'cityServed', 24, 25, 'None-label'], [7, 9, 'None-label', 'leaderName', 1, 3, 'None-label'], [7, 9, 'None-label', 'country', 10, 11, 'None-label'], [1, 3, 'None-label', 'jurisdiction', 7, 9, 'None-label']], 'pos_tags': ['DT', 'NNP', 'NNP', 'VBP', 'NN', 'IN', 'DT', 'NNP', 'NNP', 'IN', 'NNP', '.', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'NNP', 'NNP', 'CC', 'DT', 'NN', 'IN', 'NNP', 'VBZ', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', '.']}, {'tokens': ['Adirondack', 'Regional', 'Airport', 'serves', 'the', 'city', 'of', 'Lake', 'Placid', 'and', 'Saranac', 'Lake', 'in', 'New', 'York', '.', 'Saranac', 'Lake', 'is', 'part', 'of', 'Harrietstown', 'and', 'Essex', 'County', 'in', 'New', 'York', 'in', 'the', 'U.S', '.'], 'spo_list': [['Saranac Lake', 'isPartOf', 'Harrietstown'], ['Saranac Lake', 'isPartOf', 'Essex County'], ['Adirondack Regional Airport', 'cityServed', 'Lake Placid'], ['Adirondack Regional Airport', 'cityServed', 'Saranac Lake']], 'spo_details': [[10, 12, 'None-label', 'isPartOf', 21, 22, 'None-label'], [10, 12, 'None-label', 'isPartOf', 23, 25, 'None-label'], [0, 3, 'None-label', 'cityServed', 7, 9, 'None-label'], [0, 3, 'None-label', 'cityServed', 10, 12, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', '.', 'NNP', 'NNP', 'VBZ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'IN', 'DT', 'NNP', '.']}, {'tokens': ['Gordon', 'Marsden', 'is', 'the', 'leader', 'of', 'Blackpool', 'where', 'AFC', 'Blackpool', 'are', 'located', '.', 'Their', 'manager', 'is', 'Stuart', 'Parker', 'who', 'is', 'a', 'member', 'of', 'Chesterfield', 'FC', 'and', 'plays', 'for', 'KV', 'Mechelen', '.'], 'spo_list': [['Stuart Parker', 'club', 'KV Mechelen'], ['Stuart Parker', 'club', 'Chesterfield'], ['Blackpool', 'leader', 'Gordon Marsden']], 'spo_details': [[16, 18, 'None-label', 'club', 28, 30, 'None-label'], [16, 18, 'None-label', 'club', 23, 24, 'None-label'], [6, 7, 'None-label', 'leader', 0, 2, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'WRB', 'NNP', 'NNP', 'VBP', 'JJ', '.', 'PRP$', 'NN', 'VBZ', 'NNP', 'NNP', 'WP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'CC', 'VBZ', 'IN', 'NN', 'NN', '.']}, {'tokens': ['Olympiacos', 'F.C', '.', 'were', 'past', 'champions', 'in', 'the', 'Greece', 'Superleague', ',', 'the', 'league', 'AEK', 'Athens', 'FC', 'compete', 'in', '.', 'Their', 'manager', 'is', 'Gus', 'Poyet', ',', 'whose', 'club', 'is', 'Real', 'Zaragoza', 'even', 'though', 'he', 'played', 'for', 'Chelsea', 'F.C', '.'], 'spo_list': [['AEK Athens', 'league', 'Superleague'], ['Superleague', 'champions', 'Olympiacos'], ['AEK Athens', 'manager', 'Gus Poyet'], ['Gus Poyet', 'club', 'Real Zaragoza'], ['Gus Poyet', 'club', 'Chelsea']], 'spo_details': [[13, 15, 'None-label', 'league', 9, 10, 'None-label'], [9, 10, 'None-label', 'champions', 0, 1, 'None-label'], [13, 15, 'None-label', 'manager', 22, 24, 'None-label'], [22, 24, 'None-label', 'club', 28, 30, 'None-label'], [22, 24, 'None-label', 'club', 35, 36, 'None-label']], 'pos_tags': ['NNP', 'NNP', '.', 'VBD', 'JJ', 'NNS', 'IN', 'DT', 'NNP', 'NNP', ',', 'DT', 'NN', 'NNP', 'NNP', 'NNP', 'VB', 'IN', '.', 'PRP$', 'NN', 'VBZ', 'NNP', 'NNP', ',', 'WP$', 'NN', 'VBZ', 'JJ', 'NNP', 'RB', 'IN', 'PRP', 'VBD', 'IN', 'NNP', 'NNP', '.']}, {'tokens': ['Estádio', 'Municipal', 'Coaracy', 'da', 'Mata', 'Fonseca', 'is', 'the', 'name', 'of', 'the', 'ground', 'of', 'Agremiação', 'Sportiva', 'Arapiraquense', '.', 'It', 'is', 'located', 'in', 'Alagoas', '.', 'Agremiação', 'Sportiva', 'Arapiraquense', 'play', 'in', 'the', 'Campeonato', 'Brasileiro', 'Série', 'C', 'league', 'from', 'Brazil', '.', 'The', 'Vila', 'Nova', 'Futebol', 'Clube', 'were', 'champions', 'at', 'the', 'Série', 'C', '.'], 'spo_list': [['Agremiação Sportiva Arapiraquense', 'league', 'Campeonato Brasileiro Série C'], ['Campeonato Brasileiro Série C', 'country', 'Brazil'], ['Agremiação Sportiva Arapiraquense', 'ground', 'Estádio Municipal Coaracy da Mata Fonseca'], ['Estádio Municipal Coaracy da Mata Fonseca', 'location', 'Alagoas'], ['Campeonato Brasileiro Série C', 'champions', 'Vila Nova Futebol Clube']], 'spo_details': [[13, 16, 'None-label', 'league', 29, 33, 'None-label'], [29, 33, 'None-label', 'country', 35, 36, 'None-label'], [13, 16, 'None-label', 'ground', 0, 6, 'None-label'], [0, 6, 'None-label', 'location', 21, 22, 'None-label'], [29, 33, 'None-label', 'champions', 38, 42, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'NNP', 'NN', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', '.', 'PRP', 'VBZ', 'JJ', 'IN', 'NNP', '.', 'NNP', 'NNP', 'NNP', 'VB', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'NNP', '.', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', 'NNS', 'IN', 'DT', 'NN', 'NN', '.']}, {'tokens': ['Vila', 'Nova', 'Futebol', 'Clube', 'have', 'been', 'champions', 'of', 'Campeonato', 'Brasileiro', 'Série', 'C.', 'league', 'in', 'Brazil', '.', 'Agremiação', 'Sportiva', 'Arapiraquense', 'also', 'play', 'in', 'the', 'league', 'and', 'have', 'their', 'home', 'ground', 'at', 'Estádio', 'Municipal', 'Coaracy', 'da', 'Mata', 'Fonseca', 'in', 'Arapiraca', '.'], 'spo_list': [['Agremiação Sportiva Arapiraquense', 'league', 'Campeonato Brasileiro Série'], ['Campeonato Brasileiro Série', 'country', 'Brazil'], ['Agremiação Sportiva Arapiraquense', 'ground', 'Estádio Municipal Coaracy da Mata Fonseca'], ['Estádio Municipal Coaracy da Mata Fonseca', 'location', 'Arapiraca'], ['Campeonato Brasileiro Série', 'champions', 'Vila Nova Futebol Clube']], 'spo_details': [[16, 19, 'None-label', 'league', 8, 11, 'None-label'], [8, 11, 'None-label', 'country', 14, 15, 'None-label'], [16, 19, 'None-label', 'ground', 30, 36, 'None-label'], [30, 36, 'None-label', 'location', 37, 38, 'None-label'], [8, 11, 'None-label', 'champions', 0, 4, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'NNP', 'NNP', 'VBP', 'VBN', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'IN', 'NNP', '.', 'NNP', 'NNP', 'NNP', 'RB', 'VBP', 'IN', 'DT', 'NN', 'CC', 'VBP', 'PRP$', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NN', 'NNP', 'NNP', 'IN', 'NNP', '.']}, {'tokens': ['The', 'Vila', 'Nova', 'Futebol', 'Clube', 'were', 'champions', 'at', 'the', 'Campeonato', 'Brasileiro', 'Série', 'C.', 'in', 'Brazil', '.', 'Agremiação', 'Sportiva', 'Arapiraquense', 'who', 'also', 'play', 'in', 'the', 'league', 'have', '17000', 'members', 'and', 'are', 'managed', 'by', 'Vica', '.'], 'spo_list': [['Agremiação Sportiva Arapiraquense', 'league', 'Campeonato Brasileiro Série'], ['Campeonato Brasileiro Série', 'country', 'Brazil'], ['Agremiação Sportiva Arapiraquense', 'manager', 'Vica'], ['Agremiação Sportiva Arapiraquense', 'numberOfMembers', '17000'], ['Campeonato Brasileiro Série', 'champions', 'Vila Nova Futebol Clube']], 'spo_details': [[16, 19, 'None-label', 'league', 9, 12, 'None-label'], [9, 12, 'None-label', 'country', 14, 15, 'None-label'], [16, 19, 'None-label', 'manager', 32, 33, 'None-label'], [16, 19, 'None-label', 'numberOfMembers', 26, 27, 'None-label'], [9, 12, 'None-label', 'champions', 1, 5, 'None-label']], 'pos_tags': ['DT', 'NNP', 'NNP', 'NNP', 'NNP', 'VBD', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', '.', 'NNP', 'NNP', 'NNP', 'WP', 'RB', 'VBP', 'IN', 'DT', 'NN', 'VBP', 'CD', 'NNS', 'CC', 'VBP', 'VBN', 'IN', 'NNP', '.']}, {'tokens': ['Bananaman', 'was', 'shown', 'on', 'the', 'BBC', ',', 'first', 'airing', 'on', '3', 'October', '1983', 'and', 'the', 'UNK', 'broadcast', 'being', '15', 'April', '1986', '.', 'It', 'was', 'created', 'by', 'John', 'Geering', 'and', 'starred', 'Tim', 'Brooke', 'Taylor', '.'], 'spo_list': [['Bananaman', 'starring', 'Tim'], ['Bananaman', 'broadcastedBy', 'BBC'], ['Bananaman', 'creator', 'John Geering']], 'spo_details': [[0, 1, 'None-label', 'starring', 30, 31, 'None-label'], [0, 1, 'None-label', 'broadcastedBy', 5, 6, 'None-label'], [0, 1, 'None-label', 'creator', 26, 28, 'None-label']], 'pos_tags': ['NNP', 'VBD', 'VBN', 'IN', 'DT', 'NNP', ',', 'RB', 'VBG', 'IN', 'CD', 'NNP', 'CD', 'CC', 'DT', 'NNP', 'NN', 'VBG', 'CD', 'NNP', 'CD', '.', 'PRP', 'VBD', 'VBN', 'IN', 'NNP', 'NNP', 'CC', 'VBD', 'NNP', 'NNP', 'NNP', '.']}, {'tokens': ['Baymax', 'is', 'a', 'character', 'from', 'the', 'film', 'Big', 'Hero', '6', 'starring', 'Scott', 'Adsit', '.', 'He', 'was', 'created', 'by', 'Steven', 'T', 'Seagle', 'and', 'the', 'American', ',', 'Duncan', 'Rouleau', '.'], 'spo_list': [['Baymax', 'creator', 'Duncan Rouleau'], ['Baymax', 'creator', 'Steven'], ['Baymax', 'series', 'Big Hero 6'], ['Big Hero 6', 'starring', 'Scott Adsit']], 'spo_details': [[0, 1, 'None-label', 'creator', 25, 27, 'None-label'], [0, 1, 'None-label', 'creator', 18, 19, 'None-label'], [0, 1, 'None-label', 'series', 7, 10, 'None-label'], [7, 10, 'None-label', 'starring', 11, 13, 'None-label']], 'pos_tags': ['NNP', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'JJ', 'NN', 'CD', 'VBG', 'NNP', 'NNP', '.', 'PRP', 'VBD', 'VBN', 'IN', 'NNP', 'NN', 'NNP', 'CC', 'DT', 'NNP', ',', 'NNP', 'NNP', '.']}, {'tokens': ['20', 'Fenchurch', 'Street', 'is', 'located', 'within', 'the', 'United', 'Kingdom', ',', 'where', 'London', 'is', 'the', 'capital', ',', 'and', 'the', 'currency', 'is', 'pounds', 'sterling', '.', 'Boris', 'Johnson', 'is', 'the', 'leader', 'in', 'London', ',', 'and', 'one', 'of', 'the', 'leaders', 'of', 'the', 'United', 'Kingdom', 'is', 'Elizabeth', 'II', '.'], 'spo_list': [['20 Fenchurch Street', 'location', 'United Kingdom'], ['United Kingdom', 'capital', 'London'], ['London', 'leaderName', 'Boris Johnson'], ['United Kingdom', 'leaderName', 'Elizabeth II']], 'spo_details': [[0, 3, 'None-label', 'location', 7, 9, 'None-label'], [7, 9, 'None-label', 'capital', 11, 12, 'None-label'], [11, 12, 'None-label', 'leaderName', 23, 25, 'None-label'], [7, 9, 'None-label', 'leaderName', 41, 43, 'None-label']], 'pos_tags': ['CD', 'NNP', 'NNP', 'VBZ', 'JJ', 'IN', 'DT', 'NNP', 'NNP', ',', 'WRB', 'NNP', 'VBZ', 'DT', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'NNS', 'NN', '.', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ',', 'CC', 'CD', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNP', 'NNP', 'VBZ', 'NNP', 'NNP', '.']}, {'tokens': ['250', 'Delaware', 'Avenue', 'was', 'built', 'in', 'Buffalo', 'New', 'York', 'at', 'a', 'cost', 'of', '110', 'million', 'dollars', '.', 'Construction', 'began', 'in', 'January', '2014', 'and', 'the', 'building', 'has', '12', 'floors', 'with', 'an', 'area', 'of', '30843.8', 'square', 'metres', '.'], 'spo_list': [['250 Delaware Avenue', 'location', 'Buffalo'], ['250 Delaware Avenue', 'floorArea', '30843.8'], ['250 Delaware Avenue', 'floorCount', '12']], 'spo_details': [[0, 3, 'None-label', 'location', 6, 7, 'None-label'], [0, 3, 'None-label', 'floorArea', 32, 33, 'None-label'], [0, 3, 'None-label', 'floorCount', 26, 27, 'None-label']], 'pos_tags': ['CD', 'NNP', 'NNP', 'VBD', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'IN', 'CD', 'CD', 'NNS', '.', 'NN', 'VBD', 'IN', 'NNP', 'CD', 'CC', 'DT', 'NN', 'VBZ', 'CD', 'NNS', 'IN', 'DT', 'NN', 'IN', 'CD', 'JJ', 'NNS', '.']}, {'tokens': ['Rahm', 'Emanuel', 'is', 'the', 'leader', 'of', 'Chicago', ',', 'DuPage', 'County', ',', 'Illinois', ',', 'United', 'States', 'which', 'is', 'the', 'location', 'of', 'the', 'building', 'at', '300', 'North', 'LaSalle', 'which', 'has', '60', 'floors', '.'], 'spo_list': [['300 North LaSalle', 'location', 'Chicago'], ['Chicago', 'leaderName', 'Rahm Emanuel'], ['Chicago', 'isPartOf', 'DuPage County , Illinois'], ['300 North LaSalle', 'floorCount', '60'], ['Chicago', 'country', 'United States']], 'spo_details': [[23, 26, 'None-label', 'location', 6, 7, 'None-label'], [6, 7, 'None-label', 'leaderName', 0, 2, 'None-label'], [6, 7, 'None-label', 'isPartOf', 8, 12, 'None-label'], [23, 26, 'None-label', 'floorCount', 28, 29, 'None-label'], [6, 7, 'None-label', 'country', 13, 15, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ',', 'NNP', 'NNP', ',', 'NNP', ',', 'NNP', 'NNPS', 'WDT', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'CD', 'JJ', 'NNP', 'WDT', 'VBZ', 'CD', 'NNS', '.']}, {'tokens': ['3Arena', '(', 'owned', 'by', 'Live', 'Nation', 'Entertainment', ')', 'is', 'located', 'in', 'Dublin', ',', 'Republic', 'of', 'Ireland', '.', 'Dáil', 'Éireann', 'is', 'a', 'leader', 'in', 'Dublin', ',', 'which', 'is', 'part', 'of', 'Leinster', '.'], 'spo_list': [['3Arena', 'owner', 'Live Nation Entertainment'], ['Dublin', 'leaderTitle', 'Dáil Éireann'], ['Dublin', 'country', 'Republic of Ireland'], ['3Arena', 'location', 'Dublin'], ['Dublin', 'isPartOf', 'Leinster']], 'spo_details': [[0, 1, 'None-label', 'owner', 4, 7, 'None-label'], [11, 12, 'None-label', 'leaderTitle', 17, 19, 'None-label'], [11, 12, 'None-label', 'country', 13, 16, 'None-label'], [0, 1, 'None-label', 'location', 11, 12, 'None-label'], [11, 12, 'None-label', 'isPartOf', 29, 30, 'None-label']], 'pos_tags': ['NN', '-LRB-', 'VBN', 'IN', 'NNP', 'NNP', 'NNP', '-RRB-', 'VBZ', 'JJ', 'IN', 'NNP', ',', 'NNP', 'IN', 'NNP', '.', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', ',', 'WDT', 'VBZ', 'NN', 'IN', 'NNP', '.']}, {'tokens': ['Adisham', 'Hall', 'in', 'Sri', 'Lanka', 'was', 'constructed', 'between', '1927', 'and', '1931', 'at', 'St', 'Benedicts', 'Monastery', ',', 'Adisham', ',', 'Haputhale', ',', 'Sri', 'Lanka', 'in', 'the', 'Tudor', 'and', 'Jacobean', 'style', 'of', 'architecture', '.'], 'spo_list': [['Adisham Hall', 'location', 'Sri Lanka'], ['Adisham Hall', 'completionDate', '1931']], 'spo_details': [[0, 2, 'None-label', 'location', 3, 5, 'None-label'], [0, 2, 'None-label', 'completionDate', 10, 11, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'IN', 'NNP', 'NNP', 'VBD', 'VBN', 'IN', 'CD', 'CC', 'CD', 'IN', 'NNP', 'NNP', 'NNP', ',', 'NNP', ',', 'NNP', ',', 'NNP', 'NNP', 'IN', 'DT', 'NNP', 'CC', 'JJ', 'NN', 'IN', 'NN', '.']}, {'tokens': ['Adisham', 'Hall', 'is', 'located', 'in', 'Haputale', ',', 'Sri', 'Lanka', '.', 'The', 'capital', 'of', 'Sri', 'Lanka', 'is', 'Sri', 'Jayawardenepura', 'Kotte', ',', 'the', 'language', 'used', 'in', 'the', 'country', 'is', 'Tamil', 'and', 'the', 'currency', 'is', 'the', 'Sri', 'Lankan', 'Rupee', '.'], 'spo_list': [['Adisham Hall', 'country', 'Sri Lanka'], ['Sri Lanka', 'capital', 'Sri Jayawardenepura Kotte'], ['Sri Lanka', 'language', 'Tamil'], ['Sri Lanka', 'currency', 'Sri Lankan']], 'spo_details': [[0, 2, 'None-label', 'country', 7, 9, 'None-label'], [7, 9, 'None-label', 'capital', 16, 19, 'None-label'], [7, 9, 'None-label', 'language', 27, 28, 'None-label'], [7, 9, 'None-label', 'currency', 33, 35, 'None-label']], 'pos_tags': ['NNP', 'NNP', 'VBZ', 'JJ', 'IN', 'NNP', ',', 'NNP', 'NNP', '.', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'VBZ', 'NNP', 'NNP', 'NNP', ',', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'VBZ', 'NNP', 'CC', 'DT', 'NN', 'VBZ', 'DT', 'NNP', 'NNP', 'NN', '.']}]}\n",
      "Transforming data format: 500it [00:00, 241384.90it/s]\n",
      "Clean: 100%|██████████| 500/500 [00:00<00:00, 132070.79it/s]\n",
      "Transforming data format: 5019it [00:00, 426716.64it/s]\n",
      "Clean: 100%|██████████| 5019/5019 [00:00<00:00, 244403.56it/s]\n",
      "Transforming data format: 703it [00:00, 356196.63it/s]\n",
      "Clean: 100%|██████████| 703/703 [00:00<00:00, 240799.98it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_format = config[\"ori_data_format\"]\n",
    "print(ori_format)\n",
    "print(file_name2data)\n",
    "if ori_format != \"tplinker\": # if tplinker, skip transforming\n",
    "    for file_name, data in file_name2data.items():\n",
    "        if \"train\" in file_name:\n",
    "            data_type = \"train\"\n",
    "        if \"valid\" in file_name:\n",
    "            data_type = \"valid\"\n",
    "        if \"test\" in file_name:\n",
    "            data_type = \"test\"\n",
    "        data = preprocessor.transform_data(data, ori_format = ori_format, dataset_type = data_type, add_id = True)\n",
    "        file_name2data[file_name] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Clean and Add Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check token level span\n",
    "def check_tok_span(data):\n",
    "    def extr_ent(text, tok_span, tok2char_span):\n",
    "        char_span_list = tok2char_span[tok_span[0]:tok_span[1]]\n",
    "        char_span = (char_span_list[0][0], char_span_list[-1][1])\n",
    "        decoded_ent = text[char_span[0]:char_span[1]]\n",
    "        return decoded_ent\n",
    "\n",
    "    span_error_memory = set()\n",
    "    for sample in tqdm(data, desc = \"check tok spans\"):\n",
    "        text = sample[\"text\"]\n",
    "        tok2char_span = get_tok2char_span_map(text)\n",
    "        for ent in sample[\"entity_list\"]:\n",
    "            tok_span = ent[\"tok_span\"]\n",
    "            if extr_ent(text, tok_span, tok2char_span) != ent[\"text\"]:\n",
    "                span_error_memory.add(\"extr ent: {}---gold ent: {}\".format(extr_ent(text, tok_span, tok2char_span), ent[\"text\"]))\n",
    "                \n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            subj_tok_span, obj_tok_span = rel[\"subj_tok_span\"], rel[\"obj_tok_span\"]\n",
    "            if extr_ent(text, subj_tok_span, tok2char_span) != rel[\"subject\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, subj_tok_span, tok2char_span), rel[\"subject\"]))\n",
    "            if extr_ent(text, obj_tok_span, tok2char_span) != rel[\"object\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, obj_tok_span, tok2char_span), rel[\"object\"]))\n",
    "                \n",
    "    return span_error_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "clean data: 100%|██████████| 500/500 [00:00<00:00, 55831.74it/s]\n",
      "adding char level spans: 100%|██████████| 500/500 [00:00<00:00, 10599.22it/s]\n",
      "building relation type set and entity type set: 100%|██████████| 500/500 [00:00<00:00, 327117.77it/s]\n",
      "adding token level spans: 100%|██████████| 500/500 [00:00<00:00, 8618.59it/s]\n",
      "check tok spans: 100%|██████████| 500/500 [00:00<00:00, 11843.23it/s]\n",
      "clean data: 100%|██████████| 5019/5019 [00:00<00:00, 79325.38it/s]\n",
      "adding char level spans: 100%|██████████| 5019/5019 [00:00<00:00, 18377.30it/s]\n",
      "building relation type set and entity type set: 100%|██████████| 5019/5019 [00:00<00:00, 297455.34it/s]\n",
      "adding token level spans: 100%|██████████| 5019/5019 [00:00<00:00, 8471.46it/s]\n",
      "check tok spans: 100%|██████████| 5019/5019 [00:00<00:00, 12001.95it/s]\n",
      "clean data: 100%|██████████| 703/703 [00:00<00:00, 78434.70it/s]\n",
      "adding char level spans: 100%|██████████| 703/703 [00:00<00:00, 13802.03it/s]\n",
      "building relation type set and entity type set: 100%|██████████| 703/703 [00:00<00:00, 240858.99it/s]\n",
      "adding token level spans: 100%|██████████| 703/703 [00:00<00:00, 9136.98it/s]\n",
      "check tok spans: 100%|██████████| 703/703 [00:00<00:00, 11843.60it/s]{'test_data': {'miss_samples': 0, 'tok_span_error': 0},\n",
      " 'train_data': {'miss_samples': 0, 'tok_span_error': 0},\n",
      " 'valid_data': {'miss_samples': 0, 'tok_span_error': 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean, add char span, tok span\n",
    "# collect relations\n",
    "# check tok spans\n",
    "rel_set = set()\n",
    "ent_set = set()\n",
    "error_statistics = {}\n",
    "for file_name, data in file_name2data.items():\n",
    "    assert len(data) > 0\n",
    "    if \"relation_list\" in data[0]: # train or valid data\n",
    "        # rm redundant whitespaces\n",
    "        # separate by whitespaces\n",
    "        data = preprocessor.clean_data_wo_span(data, separate = config[\"separate_char_by_white\"])\n",
    "        error_statistics[file_name] = {}\n",
    "#         if file_name != \"train_data\":\n",
    "#             set_trace()\n",
    "        # add char span\n",
    "        if config[\"add_char_span\"]:\n",
    "            data, miss_sample_list = preprocessor.add_char_span(data, config[\"ignore_subword\"])\n",
    "            error_statistics[file_name][\"miss_samples\"] = len(miss_sample_list)\n",
    "            \n",
    "#         # clean\n",
    "#         data, bad_samples_w_char_span_error = preprocessor.clean_data_w_span(data)\n",
    "#         error_statistics[file_name][\"char_span_error\"] = len(bad_samples_w_char_span_error)\n",
    "                            \n",
    "        # collect relation types and entity types\n",
    "        for sample in tqdm(data, desc = \"building relation type set and entity type set\"):\n",
    "            if \"entity_list\" not in sample: # if \"entity_list\" not in sample, generate entity list with default type\n",
    "                ent_list = []\n",
    "                for rel in sample[\"relation_list\"]:\n",
    "                    ent_list.append({\n",
    "                        \"text\": rel[\"subject\"],\n",
    "                        \"type\": \"DEFAULT\",\n",
    "                        \"char_span\": rel[\"subj_char_span\"],\n",
    "                    })\n",
    "                    ent_list.append({\n",
    "                        \"text\": rel[\"object\"],\n",
    "                        \"type\": \"DEFAULT\",\n",
    "                        \"char_span\": rel[\"obj_char_span\"],\n",
    "                    })\n",
    "                sample[\"entity_list\"] = ent_list\n",
    "            \n",
    "            for ent in sample[\"entity_list\"]:\n",
    "                ent_set.add(ent[\"type\"])\n",
    "                \n",
    "            for rel in sample[\"relation_list\"]:\n",
    "                rel_set.add(rel[\"predicate\"])\n",
    "               \n",
    "        # add tok span\n",
    "        data = preprocessor.add_tok_span(data)\n",
    "\n",
    "        # check tok span\n",
    "        if config[\"check_tok_span\"]:\n",
    "            span_error_memory = check_tok_span(data)\n",
    "            if len(span_error_memory) > 0:\n",
    "                print(span_error_memory)\n",
    "            error_statistics[file_name][\"tok_span_error\"] = len(span_error_memory)\n",
    "            \n",
    "        file_name2data[file_name] = data\n",
    "pprint(error_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:valid_data is output to ../data4bert/webnlg/valid_data.json\n",
      "INFO:root:train_data is output to ../data4bert/webnlg/train_data.json\n",
      "INFO:root:test_data is output to ../data4bert/webnlg/test_data.json\n",
      "INFO:root:rel2id is output to ../data4bert/webnlg/rel2id.json\n",
      "INFO:root:ent2id is output to ../data4bert/webnlg/ent2id.json\n",
      "INFO:root:data_statistics is output to ../data4bert/webnlg/data_statistics.txt\n",
      "{'entity_type_num': 1,\n",
      " 'relation_type_num': 216,\n",
      " 'test_data': 703,\n",
      " 'train_data': 5019,\n",
      " 'valid_data': 500}\n"
     ]
    }
   ],
   "source": [
    "rel_set = sorted(rel_set)\n",
    "rel2id = {rel:ind for ind, rel in enumerate(rel_set)}\n",
    "\n",
    "ent_set = sorted(ent_set)\n",
    "ent2id = {ent:ind for ind, ent in enumerate(ent_set)}\n",
    "\n",
    "data_statistics = {\n",
    "    \"relation_type_num\": len(rel2id),\n",
    "    \"entity_type_num\": len(ent2id),\n",
    "}\n",
    "\n",
    "for file_name, data in file_name2data.items():\n",
    "    data_path = os.path.join(data_out_dir, \"{}.json\".format(file_name))\n",
    "    json.dump(data, open(data_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "    logging.info(\"{} is output to {}\".format(file_name, data_path))\n",
    "    data_statistics[file_name] = len(data)\n",
    "\n",
    "rel2id_path = os.path.join(data_out_dir, \"rel2id.json\")\n",
    "json.dump(rel2id, open(rel2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"rel2id is output to {}\".format(rel2id_path))\n",
    "\n",
    "ent2id_path = os.path.join(data_out_dir, \"ent2id.json\")\n",
    "json.dump(ent2id, open(ent2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"ent2id is output to {}\".format(ent2id_path))\n",
    "\n",
    "\n",
    "\n",
    "data_statistics_path = os.path.join(data_out_dir, \"data_statistics.txt\")\n",
    "json.dump(data_statistics, open(data_statistics_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "logging.info(\"data_statistics is output to {}\".format(data_statistics_path)) \n",
    "\n",
    "pprint(data_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate WordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    all_data = []\n",
    "    for data in list(file_name2data.values()):\n",
    "        all_data.extend(data)\n",
    "        \n",
    "    token2num = {}\n",
    "    for sample in tqdm(all_data, desc = \"Tokenizing\"):\n",
    "        text = sample['text']\n",
    "        for tok in tokenize(text):\n",
    "            token2num[tok] = token2num.get(tok, 0) + 1\n",
    "    \n",
    "    token2num = dict(sorted(token2num.items(), key = lambda x: x[1], reverse = True))\n",
    "    max_token_num = 50000\n",
    "    token_set = set()\n",
    "    for tok, num in tqdm(token2num.items(), desc = \"Filter uncommon words\"):\n",
    "        if num < 3: # filter words with a frequency of less than 3\n",
    "            continue\n",
    "        token_set.add(tok)\n",
    "        if len(token_set) == max_token_num:\n",
    "            break\n",
    "        \n",
    "    token2idx = {tok:idx + 2 for idx, tok in enumerate(sorted(token_set))}\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "#     idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    \n",
    "    dict_path = os.path.join(data_out_dir, \"token2idx.json\")\n",
    "    json.dump(token2idx, open(dict_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "    logging.info(\"token2idx is output to {}, total token num: {}\".format(dict_path, len(token2idx))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd02b904f4e79ee192e3987ad3fbb190681da4c631592edf2c3ce84992d68d61552",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}