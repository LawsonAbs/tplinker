{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../extra_data/duie/\"\n",
    "save_path = \"../ori_data/duie/\"\n",
    "train_data_path = data_path + \"train_data.json\"\n",
    "dev_data_path = data_path + \"dev_data.json\"\n",
    "test_data_path = data_path + \"test_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, \"r\", encoding = \"utf-8\") as file_in:\n",
    "        data = [json.loads(line) for line in tqdm(file_in)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "171293it [00:02, 63962.64it/s]\n",
      "20673it [00:00, 137640.96it/s]\n",
      "50583it [00:00, 366956.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(train_data_path)\n",
    "dev_data = load_data(dev_data_path)\n",
    "test_data = load_data(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/pretrains/pt/chinese_RoBERTa-wwm-ext_pytorch\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, add_special_tokens = False, do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(\"[\\U000e0000-\\U000e0010]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def clean_data(data, is_pred = False):\n",
    "    total_spo_droped_num, total_droped_sample_num = 0, 0 \n",
    "    bad_samples = []\n",
    "    for sample in tqdm(data):\n",
    "        # strip whitespaces\n",
    "        sample[\"text\"] = clean_text(sample[\"text\"].strip())\n",
    "        if is_pred:\n",
    "            continue\n",
    "            \n",
    "        entities = []\n",
    "        # strip\n",
    "        for spo in sample[\"spo_list\"]:\n",
    "            spo[\"subject\"] = spo[\"subject\"].strip()\n",
    "            spo[\"subject_type\"] = spo[\"subject_type\"].strip()  \n",
    "            spo[\"predicate\"] = spo[\"predicate\"].strip()\n",
    "            entities.append(spo[\"subject\"])\n",
    "            \n",
    "            for k, v in spo[\"object\"].items():\n",
    "                spo[\"object\"][k] = v.strip()\n",
    "                entities.append(v.strip())\n",
    "                \n",
    "            for k, v in spo[\"object_type\"].items():\n",
    "                spo[\"object_type\"][k] = v.strip()\n",
    "        \n",
    "        # guarantee entities can be tokenized correctly: add whitespace around the entities\n",
    "        entities = sorted(entities, key = lambda x: len(x), reverse = True)\n",
    "        reg_pattern = \"|\".join([re.escape(ent) for ent in entities if ent != \"\"])\n",
    "#         sample[\"text\"] = re.sub(\"({})\".format(reg_pattern), r\" \\1 \", sample[\"text\"]) # 将entity周围都用空格隔开\n",
    "        \n",
    "        # drop bad spo\n",
    "        text_tokens_join_str = \" \" + \" \".join(tokenizer.tokenize(sample[\"text\"])) + \" \"\n",
    "        for spo in sample[\"spo_list\"]:\n",
    "            bad_spo = False                    \n",
    "            for key, ent in list(spo[\"object\"].items()):\n",
    "                # remove spo with empty subj or obj\n",
    "                # remove spo that does not match text tokens\n",
    "                if ent == \"\" or \" \" + \" \".join(tokenizer.tokenize(ent)) + \" \" not in text_tokens_join_str:\n",
    "                    if key != \"@value\":\n",
    "                        del spo[\"object\"][key]\n",
    "                    else:\n",
    "                        bad_spo = True\n",
    "                        break\n",
    "                        \n",
    "            subj = spo[\"subject\"]\n",
    "            if subj == \"\" or \" \" + \" \".join(tokenizer.tokenize(subj)) + \" \" not in text_tokens_join_str:\n",
    "                bad_spo = True\n",
    "                \n",
    "            if bad_spo:\n",
    "                total_spo_droped_num += 1\n",
    "#                 set_trace()\n",
    "                bad_samples.append((spo, copy.deepcopy(sample)))\n",
    "                sample[\"spo_list\"].remove(spo)\n",
    "                break\n",
    "\n",
    "        # remove sample without spo\n",
    "        if len(sample[\"spo_list\"]) == 0:\n",
    "            data.remove(sample)\n",
    "            total_droped_sample_num += 1\n",
    "    return total_spo_droped_num, total_droped_sample_num, bad_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████▉| 171226/171293 [00:47<00:00, 3581.05it/s]\n",
      "100%|█████████▉| 20663/20673 [00:05<00:00, 3745.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_clean_log = clean_data(train_data)\n",
    "dev_clean_log = clean_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train, spo_dropped_num: 116, example_dropped_num: 67\ndev, spo_dropped_num: 15, example_dropped_num: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"train, spo_dropped_num: {}, example_dropped_num: {}\".format(train_clean_log[0], train_clean_log[1]))\n",
    "print(\"dev, spo_dropped_num: {}, example_dropped_num: {}\".format(dev_clean_log[0], dev_clean_log[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2tplinker_style(data, data_type):\n",
    "    normal_data = []\n",
    "    for idx, example in tqdm(enumerate(data), desc = \"transforming to tplinker style\"):\n",
    "        normal_example = {\n",
    "            \"id\": \"{}_{}\".format(data_type, idx),\n",
    "            \"text\": example[\"text\"]\n",
    "        }\n",
    "        rel_list, ent_list = [], []\n",
    "        for spo in example[\"spo_list\"]:\n",
    "            # relation\n",
    "            rel_list.append({\n",
    "                \"subject\": spo[\"subject\"],\n",
    "                \"object\": spo[\"object\"][\"@value\"],\n",
    "                \"predicate\": spo[\"predicate\"]\n",
    "            })\n",
    "            for k, val in spo[\"object\"].items():\n",
    "                if k != \"@value\":\n",
    "                    rel = {\n",
    "                        \"subject\": spo[\"object\"][\"@value\"],\n",
    "                        \"object\": spo[\"object\"][k],\n",
    "                        \"predicate\": k\n",
    "                    }\n",
    "#                     set_trace()\n",
    "                    rel_list.append(rel)\n",
    "                    \n",
    "            # entity\n",
    "            ent_list.append({\n",
    "                \"text\": spo[\"subject\"],\n",
    "                \"type\": spo[\"subject_type\"],\n",
    "            })\n",
    "            for k, val in spo[\"object\"].items():\n",
    "                ent_list.append({\n",
    "                    \"text\": val,\n",
    "                    \"type\": spo[\"object_type\"][k],\n",
    "                })\n",
    "        normal_example[\"relation_list\"] = rel_list\n",
    "        normal_example[\"entity_list\"] = ent_list\n",
    "        normal_data.append(normal_example)\n",
    "    return normal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "transforming to tplinker style: 171226it [00:01, 109934.90it/s]\n",
      "transforming to tplinker style: 20663it [00:00, 289009.14it/s]\n"
     ]
    }
   ],
   "source": [
    "normal_train_data = trans2tplinker_style(train_data, \"train\")\n",
    "normal_dev_data = trans2tplinker_style(dev_data, \"valid\")\n",
    "for idx, example in enumerate(test_data):\n",
    "    example[\"id\"] = \"{}_{}\".format(\"test\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in normal_train_data:\n",
    "#     for spo in example[\"relation_list\"]:\n",
    "#         if spo[\"predicate\"] == \"inWork\":\n",
    "#             print(example)\n",
    "#             print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "json.dump(normal_train_data, open(save_path + \"train_data.json\", \"w\", encoding = \"utf-8\"),ensure_ascii=False)\n",
    "json.dump(normal_dev_data, open(save_path + \"valid_data.json\", \"w\", encoding = \"utf-8\"),ensure_ascii=False)\n",
    "json.dump(test_data, open(save_path + \"test_data.json\", \"w\", encoding = \"utf-8\"),ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'text': '《邪少兵王》是冰火未央写的网络小说连载于旗峰天下',\n",
       " 'relation_list': [{'subject': '邪少兵王', 'object': '冰火未央', 'predicate': '作者'}],\n",
       " 'entity_list': [{'text': '邪少兵王', 'type': '图书作品'},\n",
       "  {'text': '冰火未央', 'type': '人物'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "normal_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in train_data:\n",
    "#     if \"自传\" in example[\"text\"]:\n",
    "#         print(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd02b904f4e79ee192e3987ad3fbb190681da4c631592edf2c3ce84992d68d61552",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}